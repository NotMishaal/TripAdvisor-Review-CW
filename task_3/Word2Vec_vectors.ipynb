{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk"
      ],
      "metadata": {
        "id": "5-tkPB9HMJgO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "pwq6d7IWsoW1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy"
      ],
      "metadata": {
        "id": "_6y8wkkTWfML"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ZGBWNSoP3NG",
        "outputId": "74d20a01-6264-4abf-de47-9e40f9f16493"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4SnKb8GusklA"
      },
      "outputs": [],
      "source": [
        "# import gensim.downloader as api\n",
        "# from gensim.models import KeyedVectors"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer"
      ],
      "metadata": {
        "id": "-4up_X59u8c4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "heDzvKIhsyqg",
        "outputId": "ac53d5c9-a871-4aa1-e79d-48e326059c25"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "drive_path = \"/content/drive/MyDrive/Year 4/NLP notebooks and data/\""
      ],
      "metadata": {
        "id": "P74aqF0js6gH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/Year 4/NLP notebooks and data/dataset_cleaned.csv')"
      ],
      "metadata": {
        "id": "fq8egS0Gs-Z7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texts = df['review_text'].astype(str).tolist()"
      ],
      "metadata": {
        "id": "TF0htqKfuvXq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_sentences(name_tokenizer = \"nltk\"):\n",
        "  if name_tokenizer == \"nltk\":\n",
        "    # Tokenize the sentences\n",
        "    tokenized_sentences = [word_tokenize(sentence.lower()) for sentence in texts]\n",
        "  elif name_tokenizer == \"gensim\":\n",
        "    tokenized_sentences = [gensim.utils.simple_preprocess(sentence) for sentence in texts]\n",
        "  elif name_tokenizer == \"keras\":\n",
        "    tokenizer = Tokenizer()\n",
        "    tokenizer.fit_on_texts(texts)\n",
        "    sequences = tokenizer.texts_to_sequences(texts)\n",
        "    word_index = tokenizer.word_index\n",
        "  elif name_tokenizer == \"spacy\":\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "    tokenized_sentences = [[token.text for token in nlp(sentence)] for sentence in texts]\n",
        "  return tokenized_sentences\n"
      ],
      "metadata": {
        "id": "sdoXyIYPTuFZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_sentences = tokenize_sentences(\"nltk\")"
      ],
      "metadata": {
        "id": "V9f08DyBWxek"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train Word2Vec model\n",
        "word2vec_model = Word2Vec(sentences=tokenized_sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
        "\n",
        "# Save the model\n",
        "word2vec_model.save(drive_path+\"word2vec_trained_100.model\")"
      ],
      "metadata": {
        "id": "_zzj9w0RMISN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w1 = \"dirty\"\n",
        "word2vec_model.wv.most_similar(positive=w1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ppwhsyDthSj4",
        "outputId": "08707a38-1624-4639-9616-0b31c336265e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('smell', 0.9732299447059631),\n",
              " ('toilet', 0.9650394916534424),\n",
              " ('damp', 0.9594857096672058),\n",
              " ('broken', 0.9593027234077454),\n",
              " ('mosquito', 0.9552099108695984),\n",
              " ('mould', 0.9540659785270691),\n",
              " ('sheet', 0.9533182978630066),\n",
              " ('carpet', 0.953170895576477),\n",
              " ('blood', 0.9519875645637512),\n",
              " ('conditioner', 0.9519443511962891)]"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "w2 = \"polite\"\n",
        "word2vec_model.wv.most_similar(positive=w2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XRxzyFk2hdnO",
        "outputId": "9208d7df-32b8-4ae2-85f2-6f04aca651ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('courteous', 0.9826430678367615),\n",
              " ('kind', 0.964293897151947),\n",
              " ('accommodating', 0.9606624245643616),\n",
              " ('professional', 0.9531459808349609),\n",
              " ('efficient', 0.9493714570999146),\n",
              " ('attentive', 0.9462657570838928),\n",
              " ('supportive', 0.9457810521125793),\n",
              " ('hospitable', 0.9382762312889099),\n",
              " ('caring', 0.9359667897224426),\n",
              " ('incredibly', 0.9277324676513672)]"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "w3 = \"shocked\"\n",
        "word2vec_model.wv.most_similar(positive=w3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UZR0PJnmhkNS",
        "outputId": "96d00b53-304c-49d0-92b9-84954d339b52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('imagine', 0.9821457266807556),\n",
              " ('previously', 0.976958155632019),\n",
              " ('expecting', 0.9717392921447754),\n",
              " ('based', 0.9710329174995422),\n",
              " ('nightmare', 0.9699057340621948),\n",
              " ('claim', 0.9667627215385437),\n",
              " ('uk', 0.9657294154167175),\n",
              " ('chain', 0.965700089931488),\n",
              " ('cry', 0.9647983312606812),\n",
              " ('frankly', 0.9645593166351318)]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Pad sequences - when training\n",
        "# max_sequence_length = max(len(seq) for seq in sequences)\n",
        "# data = pad_sequences(sequences, maxlen=max_sequence_length, padding='post')"
      ],
      "metadata": {
        "id": "KIjrWTpAvDYJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Download and Load Pre-trained Word2Vec Embeddings\n",
        "# Download the pre-trained Word2Vec model (Google News)\n",
        "# word2vec_model = api.load('word2vec-google-news-300')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LGdQc1bJvJ4m",
        "outputId": "e9867a23-a92a-4cb0-ceb1-d3ef26f2b369"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Save the entire model (for further training or use)\n",
        "# word2vec_model.save(drive_path+\"word2vec_model_300.model\")"
      ],
      "metadata": {
        "id": "fuYcfN2lWmf-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "collapsed": true,
        "outputId": "1f6a6da5-9d15-45d8-cb73-0431613a971b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gensim/utils.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, fname_or_handle, separately, sep_limit, ignore, pickle_protocol)\u001b[0m\n\u001b[1;32m    762\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 763\u001b[0;31m             \u001b[0m_pickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname_or_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpickle_protocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    764\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"saved %s object\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: file must have a 'write' attribute",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-640af0aca7e1>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Save the entire model (for further training or use)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mword2vec_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdrive_path\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"word2vec_model_300.model\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    772\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    773\u001b[0m         \"\"\"\n\u001b[0;32m--> 774\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mKeyedVectors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    775\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    776\u001b[0m     def most_similar(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gensim/utils.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, fname_or_handle, separately, sep_limit, ignore, pickle_protocol)\u001b[0m\n\u001b[1;32m    764\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"saved %s object\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# `fname_or_handle` does not have write attribute\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 766\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_smart_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname_or_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseparately\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep_limit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpickle_protocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    767\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    768\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gensim/utils.py\u001b[0m in \u001b[0;36m_smart_save\u001b[0;34m(self, fname, separately, sep_limit, ignore, pickle_protocol)\u001b[0m\n\u001b[1;32m    604\u001b[0m         \u001b[0mcompress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSaveLoad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_adapt_by_suffix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    605\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 606\u001b[0;31m         restores = self._save_specials(\n\u001b[0m\u001b[1;32m    607\u001b[0m             \u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseparately\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep_limit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    608\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gensim/utils.py\u001b[0m in \u001b[0;36m_save_specials\u001b[0;34m(self, fname, separately, sep_limit, ignore, pickle_protocol, compress, subname)\u001b[0m\n\u001b[1;32m    680\u001b[0m                         \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavez_compressed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrib\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mascontiguousarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    681\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 682\u001b[0;31m                         \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrib\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mascontiguousarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    683\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    684\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsr_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsc_matrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mattrib\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mignore\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(file, arr, allow_pickle, fix_imports)\u001b[0m\n\u001b[1;32m    544\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mfile_ctx\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfid\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    545\u001b[0m         \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masanyarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 546\u001b[0;31m         format.write_array(fid, arr, allow_pickle=allow_pickle,\n\u001b[0m\u001b[1;32m    547\u001b[0m                            pickle_kwargs=dict(fix_imports=fix_imports))\n\u001b[1;32m    548\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/lib/format.py\u001b[0m in \u001b[0;36mwrite_array\u001b[0;34m(fp, array, version, allow_pickle, pickle_kwargs)\u001b[0m\n\u001b[1;32m    728\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    729\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misfileobj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 730\u001b[0;31m             \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtofile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    731\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    732\u001b[0m             for chunk in numpy.nditer(\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get vectors for our dataframe text\n",
        "# Function to get vector for a tokenized sentence\n",
        "def get_text_vector(tokens, model, embedding_dim):\n",
        "    word_vectors = [model.wv[word] for word in tokens if word in model.wv]\n",
        "    if not word_vectors:\n",
        "        return np.zeros(embedding_dim)\n",
        "    return np.mean(word_vectors, axis=0)"
      ],
      "metadata": {
        "id": "QjMxnqHWC-_G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get vectors for each sentence\n",
        "embedding_dim = word2vec_model.vector_size\n",
        "sentence_vectors = [get_text_vector(sentence, word2vec_model, embedding_dim) for sentence in tokenized_sentences]\n"
      ],
      "metadata": {
        "id": "2j4fIWytDljT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T8HH8wgJERrj",
        "outputId": "22a36da8-96cf-4184-a1e3-4e50b4a93bff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "100"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence_vectors[0:2]"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FU3ZuHCXES3q",
        "outputId": "5fa9796b-eb55-423d-a57d-4b90e4ce2368"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([ 0.37622884,  0.2874541 ,  0.16334994, -0.13454556, -0.7014998 ,\n",
              "        -0.41706085,  0.27231303,  0.5167258 , -0.5700409 , -0.16082491,\n",
              "        -0.4041826 , -0.5949401 , -0.09167208, -0.27466404, -0.39891988,\n",
              "        -0.13725099,  0.9696409 , -0.34481433,  0.25778148, -1.2668511 ,\n",
              "        -0.20765147,  0.27817032,  0.11776184, -0.5156096 ,  0.03768031,\n",
              "         0.15127067, -0.76759785,  0.12593612,  0.15623406,  0.86069286,\n",
              "         0.67844576, -0.06635437,  0.6754332 , -0.38496345, -0.6005006 ,\n",
              "         0.71387005, -0.3267343 , -0.09896508, -0.39583242, -0.8355282 ,\n",
              "         0.7062566 , -0.36886048, -0.33818424,  0.1297102 ,  0.40202788,\n",
              "        -0.17005341,  0.22881562, -0.762382  ,  0.04284612,  0.54583436,\n",
              "         0.1426572 ,  0.03337465,  0.36548212, -0.4472331 , -0.35628584,\n",
              "         0.0686673 , -0.09136609,  0.20556083, -0.30156627, -0.37138572,\n",
              "         0.1677826 , -0.5136862 ,  0.4626584 ,  0.4421136 , -0.21006285,\n",
              "         0.32541886,  0.76497555,  0.7261552 , -0.5651388 ,  0.34221646,\n",
              "        -0.28521362, -0.03049681,  0.65402514,  0.7392803 ,  0.38706282,\n",
              "         0.6058241 ,  0.00183946, -0.02979491, -0.52219784,  0.61307913,\n",
              "        -0.38232565, -0.270073  , -0.20101288,  0.7611371 , -0.2808721 ,\n",
              "        -0.44439974,  0.89100707,  0.21742193,  0.26617423,  0.34576404,\n",
              "         0.54373527,  0.11514588, -0.18834977, -0.18049474, -0.03863408,\n",
              "        -0.04904576,  0.16912664,  0.2707191 ,  0.68723786, -0.14340906],\n",
              "       dtype=float32),\n",
              " array([ 2.25999802e-02,  1.41366452e-01,  1.36234522e-01,  1.11022418e-04,\n",
              "        -2.04943240e-01, -4.60172415e-01,  3.10935706e-01,  4.60881472e-01,\n",
              "        -3.77151370e-01, -1.08413741e-01, -3.79313797e-01, -2.06279337e-01,\n",
              "         9.53578576e-03, -3.11614722e-02, -4.93515693e-02, -8.92726406e-02,\n",
              "        -9.75746214e-02, -2.71781027e-01, -2.05931768e-01, -6.50112987e-01,\n",
              "         2.69212782e-01,  5.28280400e-02, -2.91439779e-02,  2.28289396e-01,\n",
              "         6.48118109e-02, -6.39665499e-02, -1.92892969e-01, -2.64391273e-01,\n",
              "        -2.87130296e-01,  3.80621284e-01,  2.88665146e-01,  5.07112108e-02,\n",
              "         1.71819553e-01, -1.49136290e-01, -3.77908200e-02,  3.35699439e-01,\n",
              "        -2.26384506e-01, -1.71940789e-01, -9.22073200e-02, -7.37844169e-01,\n",
              "        -3.25193182e-02, -3.35703075e-01, -2.37537950e-01, -6.23561367e-02,\n",
              "        -5.96174151e-02, -2.36566454e-01, -1.54194772e-01, -4.86884490e-02,\n",
              "        -6.89608902e-02,  3.26651037e-01,  1.94246829e-01, -1.53964326e-01,\n",
              "        -2.62311667e-01, -2.68729359e-01,  8.29356164e-03,  1.61095679e-01,\n",
              "         2.45893031e-01,  1.06201945e-02, -3.66377831e-01,  1.25614956e-01,\n",
              "        -1.38162058e-02, -1.53043419e-01, -2.41510421e-01, -1.10861458e-01,\n",
              "        -3.61893952e-01,  6.70217812e-01,  4.51191604e-01,  1.79312438e-01,\n",
              "        -2.26484492e-01,  3.95462722e-01, -2.73044735e-01,  2.16192752e-02,\n",
              "         2.70670056e-01,  1.86048955e-01,  1.78195074e-01,  3.02746356e-01,\n",
              "        -1.34601872e-02,  4.04687785e-02, -2.68185288e-01, -1.05259359e-01,\n",
              "        -1.73188522e-01,  7.11257532e-02, -2.85048634e-01,  3.84318531e-01,\n",
              "        -3.52247000e-01,  8.19292441e-02,  8.56225342e-02,  1.42584309e-01,\n",
              "         3.13648522e-01,  6.95251226e-02,  3.87816489e-01,  3.26593816e-01,\n",
              "         2.31869705e-02,  3.30759883e-01,  4.47953284e-01,  4.72727679e-02,\n",
              "         2.20143095e-01, -3.50698918e-01,  1.20005436e-01, -2.06931829e-01],\n",
              "       dtype=float32)]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the NumPy array as a .npy file\n",
        "np.save(drive_path+'/dense_vectors/word2vec_vectors.npy', np.array(sentence_vectors))"
      ],
      "metadata": {
        "id": "m-0LECgYU-oK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example DataFrame to store sentence vectors\n",
        "df['w2v_sent_vectors'] = sentence_vectors\n",
        "\n"
      ],
      "metadata": {
        "id": "cVLqgwAAEmJG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['w2v_sent_vectors'].head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "id": "X65RTJwzEwEz",
        "outputId": "0aa0be9b-d281-45a5-853c-43c5ea25bb63"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    [0.31044984, 0.4014417, 0.083564386, -0.276954...\n",
              "1    [0.021798797, 0.116291, 0.17720811, 0.09335749...\n",
              "2    [0.4899982, 0.37943998, 0.29790735, -0.0875043...\n",
              "3    [0.116864815, 0.21760912, -0.11814022, -0.4093...\n",
              "4    [0.26037872, 0.18244962, 0.122816935, -0.10558...\n",
              "Name: w2v_sent_vectors, dtype: object"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>w2v_sent_vectors</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[0.31044984, 0.4014417, 0.083564386, -0.276954...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[0.021798797, 0.116291, 0.17720811, 0.09335749...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[0.4899982, 0.37943998, 0.29790735, -0.0875043...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[0.116864815, 0.21760912, -0.11814022, -0.4093...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[0.26037872, 0.18244962, 0.122816935, -0.10558...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> object</label>"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv(drive_path+'w2v_sent_added.csv', index = False)"
      ],
      "metadata": {
        "id": "v9EWWipAE_GJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df['w2v_100_vectors'] = df['text'].apply(lambda x: get_text_vector(x, model, embedding_dim))\n",
        "\n",
        "# # Optionally, remove rows with empty vectors if you don't want them in the dataset\n",
        "# data = data.dropna(subset=['vectors'])\n",
        "\n",
        "# # Now, the 'vectors' column contains the average word vectors for each text\n",
        "# print(data.head())"
      ],
      "metadata": {
        "id": "SFB4TVImDGL1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the embedding matrix to a file\n",
        "# np.save(drive_path+'Word2Vec_embedding_matrix_300.npy', embedding_matrix)"
      ],
      "metadata": {
        "id": "IzmE-Feava4N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the embedding matrix from the file\n",
        "# embedding_matrix = np.load(drive_path+'Word2Vec_embedding_matrix_300.npy')"
      ],
      "metadata": {
        "id": "uQMYyh9tvf_a"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}