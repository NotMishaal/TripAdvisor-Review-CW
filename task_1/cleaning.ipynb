{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk import WordNetLemmatizer, word_tokenize, ngrams\n",
    "from nltk.tokenize import WordPunctTokenizer, word_tokenize, RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import spacy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from langdetect import detect\n",
    "import emoji\n",
    "from transformers import BertTokenizer\n",
    "import sentencepiece as spm\n",
    "import inflect\n",
    "import contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE, WordPiece\n",
    "from tokenizers.trainers import BpeTrainer, WordPieceTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Import & Initial Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the raw individual datasets\n",
    "colombo = pd.read_csv('../data/raw/dataset_colombo_raw.csv')\n",
    "east = pd.read_csv('../data/raw/dataset_east_raw.csv')\n",
    "ella = pd.read_csv('../data/raw/dataset_ella_raw.csv')\n",
    "galle = pd.read_csv('../data/raw/dataset_galle_raw.csv')\n",
    "kandy = pd.read_csv('../data/raw/dataset_kandy_raw_1.csv')\n",
    "nuwaraeliya = pd.read_csv('../data/raw/dataset_nuwaraeliya_raw.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the shape of the datasets\n",
    "colombo.shape, east.shape, ella.shape, galle.shape, kandy.shape, nuwaraeliya.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking which columns to keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kandy.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kandy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# based on manual analysis of the scrapped data the following columns were deemed relevant\n",
    "useful_columns = [\n",
    "    'helpfulVotes',\n",
    "    'id',\n",
    "    'placeInfo/address',\n",
    "    'placeInfo/addressObj/city',\n",
    "    'placeInfo/name',\n",
    "    'placeInfo/numberOfReviews',\n",
    "    'placeInfo/rating',\n",
    "    'placeInfo/webUrl',\n",
    "    'publishedDate',\n",
    "    'rating',\n",
    "    'roomTip',\n",
    "    'text',\n",
    "    'title',\n",
    "    'travelDate',\n",
    "    'tripType',\n",
    "    'user/username',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter dataframes to only include useful columns\n",
    "colombo = colombo[useful_columns]\n",
    "east = east[useful_columns]\n",
    "ella = ella[useful_columns]\n",
    "galle = galle[useful_columns]\n",
    "kandy = kandy[useful_columns]\n",
    "nuwaraeliya = nuwaraeliya[useful_columns]\n",
    "\n",
    "# check shape of dataframes after filteration\n",
    "colombo.shape, east.shape, ella.shape, galle.shape, kandy.shape, nuwaraeliya.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate dataframes\n",
    "data = pd.concat([colombo, east, ella, galle, kandy, nuwaraeliya])\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# renaming columns to be more readable\n",
    "column_names = {\n",
    "    'helpfulVotes': 'helpfulVotes',\n",
    "    'id': 'id',\n",
    "    'placeInfo/address': 'address',\n",
    "    'placeInfo/addressObj/city': 'city',\n",
    "    'placeInfo/name': 'placeName',\n",
    "    'placeInfo/numberOfReviews': 'numberOfReviews',\n",
    "    'placeInfo/rating': 'placeRating',\n",
    "    'placeInfo/webUrl': 'webUrl',\n",
    "    'publishedDate': 'publishedDate',\n",
    "    'rating': 'userRating',\n",
    "    'roomTip': 'roomTip',\n",
    "    'text': 'review_text',\n",
    "    'title': 'review_title',\n",
    "    'travelDate': 'travelDate',\n",
    "    'tripType': 'tripType',\n",
    "    'user/username': 'username',\n",
    "}\n",
    "\n",
    "data = data.rename(columns=column_names)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.to_csv('..data/dataset_raw.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/dataset_raw.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify incomplete reviews\n",
    "print(\"Number of incomplete reviews:\", df[df['review_text'].isnull()].shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicates = df.duplicated(subset=['review_text'])\n",
    "print(\"Number of duplicate reviews removed:\", df[df.duplicated(subset=['review_text'])].shape[0])\n",
    "\n",
    "df = df[~duplicates]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for missing data\n",
    "print(f\"Number of missing data: \\n{df.isnull().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check null rows for review_text column\n",
    "df[df['review_text'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop null row\n",
    "df.dropna(subset=['review_text'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert text to lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['review_text'] = df['review_text'].str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove HTML tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['review_text'] = df['review_text'].str.replace(r'<.*?>', ' ', regex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove URLs/emails from reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['review_text'] = df['review_text'].str.replace(r'http\\S+|www.\\S+|\\S+@\\S+', ' ', regex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove punctuation & special characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['review_text'] = df['review_text'].str.replace(r'[^\\w\\s]', ' ', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the reviews for emojis using emoji library\n",
    "def contains_emoji(text):\n",
    "    return emoji.emoji_count(text) > 0\n",
    "\n",
    "print(f\"Review titles with emojis: {df['review_title'].apply(contains_emoji).sum()}\")\n",
    "print(f\"Review text with emojis: {df['review_text'].apply(contains_emoji).sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# handling line breaks\n",
    "lb_count = df['review_text'].str.contains('\\n').sum()\n",
    "\n",
    "df['review_text'] = df['review_text'].str.replace('\\n', ' ')\n",
    "\n",
    "print(f\"Number of line breaks before: {lb_count}\")\n",
    "print(\"Number of line breaks after: \", df['review_text'].str.contains('\\n').sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# expand contractions\n",
    "df['review_text'] = df['review_text'].apply(lambda x: contractions.fix(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dealing with numbers (inflecting numbers to words) \n",
    "p = inflect.engine()\n",
    "\n",
    "# Function to replace numbers with words\n",
    "def convert_numbers_to_words(text):\n",
    "    return ' '.join([p.number_to_words(word) if word.isdigit() else word for word in text.split()])\n",
    "\n",
    "df['review_text'] = df['review_text'].apply(convert_numbers_to_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check stopword count in review_text column\n",
    "pre_sw_count = df['review_text'].apply(lambda x: len([w for w in x.split() if w in stop_words])).sum()\n",
    "df['review_text'] = df['review_text'].apply(lambda x: ' '.join([word for word in x.split() if word not in stop_words]))\n",
    "\n",
    "post_sw_count = df['review_text'].apply(lambda x: len([w for w in x.split() if w in stop_words])).sum()\n",
    "\n",
    "print(f\"Number of stop words before: {pre_sw_count}\")\n",
    "print(f\"Number of stop words after: {post_sw_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatize Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "df['review_text'] = df['review_text'].apply(lambda x: ' '.join([lemmatizer.lemmatize(word) for word in x.split()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Multilingual Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect language\n",
    "df['language'] = df['review_text'].apply(detect)\n",
    "df['language'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.width', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['language'] != 'en', ['id', 'review_text', 'language']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep english reviews only\n",
    "df = df[df['language'] == 'en']\n",
    "df.drop(columns=['language'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize Whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['review_text'] = df['review_text'].apply(lambda x: ' '.join(x.split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Techniques explored:\n",
    "1. Word Tokenization\n",
    "2. Subword Tokenization (Byte-pair or BPE)\n",
    "3. n-Gram Tokenization (bi-gram/tri-gram)\n",
    "4. Whitespace Tokenization\n",
    "5. Rule-Based Tokenization\n",
    "6. SpaCy Tokenization\n",
    "7. WordPiece Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tkn_results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_vocab_size(tokenized_column):\n",
    "    vocab_size = len(set([token for tokens in tokenized_column for token in tokens]))\n",
    "    return vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_average_token_length(tokenized_column):\n",
    "    total_token_length = sum(len(token) for tokens in tokenized_column for token in tokens)\n",
    "    total_tokens = sum(len(tokens) for tokens in tokenized_column)\n",
    "    average_token_length = total_token_length / total_tokens\n",
    "    return average_token_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_tokenizer_metrics(tokenized_column, tokenizer_name):\n",
    "    vocab_size = len(set([token for tokens in tokenized_column for token in tokens]))\n",
    "    avg_tkn_len = calculate_average_token_length(tokenized_column)\n",
    "\n",
    "    tkn_results.append({\n",
    "        \"Tokenizer\": tokenizer_name,\n",
    "        \"Vocabulary Size\": vocab_size,\n",
    "        \"Average Token Length\": avg_tkn_len,\n",
    "    })\n",
    "\n",
    "    print(f'Vocab size: {vocab_size}')\n",
    "    print(f'Average token length: {avg_tkn_len}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df['word_tokens'] = df['review_text'].apply(lambda x: word_tokenize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_tokenizer_metrics(df['word_tokens'], \"Word Tokenization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BPE Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bpe_tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
    "trainer = BpeTrainer(special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"])\n",
    "bpe_tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "# Train on a sample of the data\n",
    "corpus = df['review_text'].astype(str).tolist()\n",
    "bpe_tokenizer.train_from_iterator(corpus, trainer)\n",
    "\n",
    "df['bpe_tokens'] = df['review_text'].apply(\n",
    "    lambda x: bpe_tokenizer.encode(str(x)).tokens\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_tokenizer_metrics(df['bpe_tokens'], \"BPE Tokenization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### n-Gram Tokenization\n",
    "\n",
    "Bigram & Trigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# bigram tokenization\n",
    "df['bigram_tokens'] = df['word_tokens'].apply(lambda x: list(ngrams(x, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_tokenizer_metrics(df['bigram_tokens'], \"Bigram Tokenization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# trigram tokenization\n",
    "df['trigram_tokens'] = df['word_tokens'].apply(lambda x: list(ngrams(x, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_tokenizer_metrics(df['trigram_tokens'], \"Trigram Tokenization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Whitespace Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df['whitespace_tokens'] = df['review_text'].apply(lambda x: str(x).split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_tokenizer_metrics(df['whitespace_tokens'], \"Whitespace Tokenization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if whitespace tokenization is the same as word tokenization\n",
    "df[df['word_tokens'] != df['whitespace_tokens']]['word_tokens'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rule Based Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "rule_tokenizer = RegexpTokenizer(r'\\w+')\n",
    "df['rule_based_tokens'] = df['review_text'].apply(lambda x: rule_tokenizer.tokenize(str(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_tokenizer_metrics(df['rule_based_tokens'], \"Rule Based Tokenization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SpaCy Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spacy.cli.download(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "df['spacy_tokens'] = df['review_text'].apply(lambda x: [token.text for token in nlp(str(x))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_tokenizer_metrics(df['spacy_tokens'], \"SpaCy Tokenization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wordpiece Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "wordpiece_tokenizer = Tokenizer(WordPiece(unk_token=\"[UNK]\"))\n",
    "\n",
    "wordpiece_trainer = WordPieceTrainer(special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"],\n",
    "    vocab_size=20000\n",
    ")\n",
    "wordpiece_tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "# Train on the corpus\n",
    "wordpiece_tokenizer.train_from_iterator(corpus, wordpiece_trainer)\n",
    "\n",
    "df['wordpiece_tokens'] = df['review_text'].apply(lambda x: wordpiece_tokenizer.encode(str(x)).tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_tokenizer_metrics(df['wordpiece_tokens'], \"Wordpiece Tokenization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(tkn_results)\n",
    "results_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exporting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.reset_option('^display')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('../data/dataset_cleaned.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
